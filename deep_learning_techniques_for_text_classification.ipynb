{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!conda install -y gdown"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:57:18.306546Z",
          "iopub.execute_input": "2023-02-20T16:57:18.307230Z"
        },
        "trusted": true,
        "id": "Vkr3QIo7wv3W",
        "outputId": "8bd86fce-9d1a-4a13-9f8a-f123042fedbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 14mto87jQZLt4hTOcVV1H3wIyF1cvS4fS"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:24:39.971544Z",
          "iopub.execute_input": "2023-02-20T16:24:39.972141Z",
          "iopub.status.idle": "2023-02-20T16:24:47.478294Z",
          "shell.execute_reply.started": "2023-02-20T16:24:39.972098Z",
          "shell.execute_reply": "2023-02-20T16:24:47.476753Z"
        },
        "trusted": true,
        "id": "xLUjF3jgwv3a",
        "outputId": "cd467859-f4fd-470b-b556-4d46593993c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "/bin/bash: /opt/conda/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n/opt/conda/lib/python3.7/site-packages/gdown/cli.py:125: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  category=FutureWarning,\nDownloading...\nFrom: https://drive.google.com/uc?id=14mto87jQZLt4hTOcVV1H3wIyF1cvS4fS\nTo: /kaggle/working/glove.6B.50d.txt\n100%|████████████████████████████████████████| 171M/171M [00:04<00:00, 37.7MB/s]\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential,Model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from keras.layers import Dropout, Dense, GRU, Embedding,Input,Flatten, MaxPooling1D, Conv1D, Activation\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from keras.layers import Concatenate\n",
        "from keras.preprocessing import sequence"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:05.211559Z",
          "iopub.execute_input": "2023-02-20T16:26:05.212141Z",
          "iopub.status.idle": "2023-02-20T16:26:05.221455Z",
          "shell.execute_reply.started": "2023-02-20T16:26:05.212098Z",
          "shell.execute_reply": "2023-02-20T16:26:05.219499Z"
        },
        "trusted": true,
        "id": "Ub7FmcJWwv3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "X_train = newsgroups_train.data\n",
        "X_test = newsgroups_test.data\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:14.408868Z",
          "iopub.execute_input": "2023-02-20T16:26:14.409350Z",
          "iopub.status.idle": "2023-02-20T16:26:27.074178Z",
          "shell.execute_reply.started": "2023-02-20T16:26:14.409316Z",
          "shell.execute_reply": "2023-02-20T16:26:27.072847Z"
        },
        "trusted": true,
        "id": "mO_D5ZIpwv3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TFIDF(X_train, X_test,MAX_NB_WORDS=75000):\n",
        "    vectorizer_x = TfidfVectorizer(max_features=MAX_NB_WORDS)\n",
        "    X_train = vectorizer_x.fit_transform(X_train).toarray()\n",
        "    X_test = vectorizer_x.transform(X_test).toarray()\n",
        "    print(\"tf-idf with\",str(np.array(X_train).shape[1]),\"features\")\n",
        "    return (X_train,X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:27.076340Z",
          "iopub.execute_input": "2023-02-20T16:26:27.076736Z",
          "iopub.status.idle": "2023-02-20T16:26:27.084418Z",
          "shell.execute_reply.started": "2023-02-20T16:26:27.076702Z",
          "shell.execute_reply": "2023-02-20T16:26:27.082477Z"
        },
        "trusted": true,
        "id": "fGHychKFwv3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadData_Tokenizer(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train = text[0:len(X_train), ]\n",
        "    X_test = text[len(X_train):, ]\n",
        "    embeddings_index = {}\n",
        "    f = open(\"/kaggle/working/glove.6B.50d.txt\", encoding=\"utf8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_index))\n",
        "    return (X_train, X_test, word_index,embeddings_index)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:27.085735Z",
          "iopub.execute_input": "2023-02-20T16:26:27.086069Z",
          "iopub.status.idle": "2023-02-20T16:26:27.135531Z",
          "shell.execute_reply.started": "2023-02-20T16:26:27.086038Z",
          "shell.execute_reply": "2023-02-20T16:26:27.134208Z"
        },
        "trusted": true,
        "id": "72xfr_Swwv3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DNN"
      ],
      "metadata": {
        "id": "wfWWWWQZwv3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf, X_test_tfidf = TFIDF(X_train,X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:27.137824Z",
          "iopub.execute_input": "2023-02-20T16:26:27.138893Z",
          "iopub.status.idle": "2023-02-20T16:26:45.601023Z",
          "shell.execute_reply.started": "2023-02-20T16:26:27.138854Z",
          "shell.execute_reply": "2023-02-20T16:26:45.599252Z"
        },
        "trusted": true,
        "id": "zThVK3Jlwv3g",
        "outputId": "84ebe367-3baa-42a8-d360-7c89e4649058"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "tf-idf with 75000 features\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Build_Model_DNN_Text(shape, nClasses, dropout=0.5):\n",
        "    \"\"\"\n",
        "    buildModel_DNN_Tex(shape, nClasses,dropout)\n",
        "    Build Deep neural networks Model for text classification\n",
        "    Shape is input feature space\n",
        "    nClasses is number of classes\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    node = 512 # number of nodes\n",
        "    nLayers = 4 # number of  hidden layer\n",
        "    model.add(Dense(node,input_dim=shape,activation='relu'))\n",
        "    model.add(Dropout(dropout))\n",
        "    for i in range(0,nLayers):\n",
        "        model.add(Dense(node,input_dim=node,activation='relu'))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Dense(nClasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "model_DNN = Build_Model_DNN_Text(X_train_tfidf.shape[1], 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:26:45.602965Z",
          "iopub.execute_input": "2023-02-20T16:26:45.603503Z",
          "iopub.status.idle": "2023-02-20T16:26:46.136820Z",
          "shell.execute_reply.started": "2023-02-20T16:26:45.603455Z",
          "shell.execute_reply": "2023-02-20T16:26:46.135525Z"
        },
        "trusted": true,
        "id": "xQRhU4zYwv3h",
        "outputId": "c4873fef-2136-4301-bb98-9679174de0ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2023-02-20 16:26:45.648476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64::/opt/conda/lib\n2023-02-20 16:26:45.648619: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_DNN.summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:32:42.195133Z",
          "iopub.execute_input": "2023-02-20T16:32:42.195877Z",
          "iopub.status.idle": "2023-02-20T16:32:42.244960Z",
          "shell.execute_reply.started": "2023-02-20T16:32:42.195818Z",
          "shell.execute_reply": "2023-02-20T16:32:42.243291Z"
        },
        "trusted": true,
        "id": "1fZNcb1Gwv3i",
        "outputId": "c00d29be-a4d1-4232-af3b-9a5f82e1868c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 512)               38400512  \n                                                                 \n dropout (Dropout)           (None, 512)               0         \n                                                                 \n dense_1 (Dense)             (None, 512)               262656    \n                                                                 \n dropout_1 (Dropout)         (None, 512)               0         \n                                                                 \n dense_2 (Dense)             (None, 512)               262656    \n                                                                 \n dropout_2 (Dropout)         (None, 512)               0         \n                                                                 \n dense_3 (Dense)             (None, 512)               262656    \n                                                                 \n dropout_3 (Dropout)         (None, 512)               0         \n                                                                 \n dense_4 (Dense)             (None, 512)               262656    \n                                                                 \n dropout_4 (Dropout)         (None, 512)               0         \n                                                                 \n dense_5 (Dense)             (None, 20)                10260     \n                                                                 \n=================================================================\nTotal params: 39,461,396\nTrainable params: 39,461,396\nNon-trainable params: 0\n_________________________________________________________________\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_DNN.fit(X_train_tfidf, y_train,\n",
        "                              validation_data=(X_test_tfidf, y_test),\n",
        "                              epochs=10,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:32:46.818372Z",
          "iopub.execute_input": "2023-02-20T16:32:46.819911Z",
          "iopub.status.idle": "2023-02-20T16:38:37.507040Z",
          "shell.execute_reply.started": "2023-02-20T16:32:46.819848Z",
          "shell.execute_reply": "2023-02-20T16:38:37.505335Z"
        },
        "trusted": true,
        "id": "yD-6jAagwv3j",
        "outputId": "970da42f-f604-4043-f5fb-4bc242e95068"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2023-02-20 16:32:48.359508: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3394200000 exceeds 10% of free system memory.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/10\n89/89 - 40s - loss: 2.7882 - accuracy: 0.1012 - val_loss: 2.0328 - val_accuracy: 0.3295 - 40s/epoch - 445ms/step\nEpoch 2/10\n89/89 - 34s - loss: 1.3998 - accuracy: 0.5040 - val_loss: 0.9658 - val_accuracy: 0.6806 - 34s/epoch - 383ms/step\nEpoch 3/10\n89/89 - 34s - loss: 0.6009 - accuracy: 0.7873 - val_loss: 0.7452 - val_accuracy: 0.7966 - 34s/epoch - 383ms/step\nEpoch 4/10\n89/89 - 34s - loss: 0.2552 - accuracy: 0.9168 - val_loss: 0.8428 - val_accuracy: 0.7889 - 34s/epoch - 383ms/step\nEpoch 5/10\n89/89 - 34s - loss: 0.1356 - accuracy: 0.9609 - val_loss: 0.9114 - val_accuracy: 0.8008 - 34s/epoch - 384ms/step\nEpoch 6/10\n89/89 - 34s - loss: 0.0807 - accuracy: 0.9780 - val_loss: 0.8862 - val_accuracy: 0.8101 - 34s/epoch - 382ms/step\nEpoch 7/10\n89/89 - 34s - loss: 0.0650 - accuracy: 0.9834 - val_loss: 0.9110 - val_accuracy: 0.8157 - 34s/epoch - 384ms/step\nEpoch 8/10\n89/89 - 34s - loss: 0.0458 - accuracy: 0.9874 - val_loss: 0.9179 - val_accuracy: 0.8201 - 34s/epoch - 380ms/step\nEpoch 9/10\n89/89 - 34s - loss: 0.0369 - accuracy: 0.9903 - val_loss: 0.9690 - val_accuracy: 0.8108 - 34s/epoch - 382ms/step\nEpoch 10/10\n89/89 - 34s - loss: 0.0273 - accuracy: 0.9920 - val_loss: 1.0838 - val_accuracy: 0.8082 - 34s/epoch - 381ms/step\n",
          "output_type": "stream"
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<keras.callbacks.History at 0x7fb56bd323d0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model_DNN.predict(X_test_tfidf).argmax(axis=1)\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:42:24.321733Z",
          "iopub.execute_input": "2023-02-20T16:42:24.323399Z",
          "iopub.status.idle": "2023-02-20T16:42:38.678614Z",
          "shell.execute_reply.started": "2023-02-20T16:42:24.323283Z",
          "shell.execute_reply": "2023-02-20T16:42:38.677300Z"
        },
        "trusted": true,
        "id": "qGdyFix4wv3k",
        "outputId": "4aafbd4b-70e9-4c57-9736-0a48716b0ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "236/236 [==============================] - 11s 46ms/step\n              precision    recall  f1-score   support\n\n           0       0.82      0.73      0.77       319\n           1       0.56      0.84      0.67       389\n           2       0.77      0.68      0.73       394\n           3       0.61      0.77      0.68       392\n           4       0.75      0.82      0.78       385\n           5       0.89      0.63      0.74       395\n           6       0.86      0.78      0.82       390\n           7       0.81      0.92      0.86       396\n           8       0.99      0.87      0.93       398\n           9       0.97      0.92      0.95       397\n          10       0.98      0.94      0.96       399\n          11       0.92      0.90      0.91       396\n          12       0.67      0.73      0.70       393\n          13       0.90      0.72      0.80       396\n          14       0.91      0.87      0.89       394\n          15       0.88      0.87      0.87       398\n          16       0.77      0.89      0.83       364\n          17       0.96      0.91      0.93       376\n          18       0.81      0.61      0.70       310\n          19       0.56      0.67      0.61       251\n\n    accuracy                           0.81      7532\n   macro avg       0.82      0.80      0.81      7532\nweighted avg       0.83      0.81      0.81      7532\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "uLlbLz-gwv3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-20T16:57:07.993540Z",
          "iopub.execute_input": "2023-02-20T16:57:07.995458Z",
          "iopub.status.idle": "2023-02-20T16:57:08.133464Z",
          "shell.execute_reply.started": "2023-02-20T16:57:07.995117Z",
          "shell.execute_reply": "2023-02-20T16:57:08.131394Z"
        },
        "trusted": true,
        "id": "-9hqBMvzwv3l",
        "outputId": "681e23c6-3d81-44c0-94a7-80d34c8bf119"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_9292/1916970170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_Glove\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_Glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadData_Tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'loadData_Tokenizer' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'loadData_Tokenizer' is not defined",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Build_Model_RNN_Text(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "    \"\"\"\n",
        "    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "    word_index in word index ,\n",
        "    embeddings_index is embeddings index, \n",
        "    nClasses is number of classes,\n",
        "    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    hidden_layer = 3\n",
        "    gru_node = 32\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    print(gru_node)\n",
        "    for i in range(0,hidden_layer):\n",
        "        model.add(GRU(gru_node,return_sequences=True, recurrent_dropout=0.2))\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(GRU(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "ne6C9K7Mwv3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RNN = Build_Model_RNN_Text(word_index,embeddings_index, 20)"
      ],
      "metadata": {
        "trusted": true,
        "id": "V-lUXbLawv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RNN.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "bUgsSuegwv3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RNN.fit(X_train_Glove, y_train,\n",
        "                              validation_data=(X_test_Glove, y_test),\n",
        "                              epochs=20,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "T31nwQJowv3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model_RNN.predict(X_test_Glove)\n",
        "print(metrics.classification_report(y_test,np.argmax(predicted,axis=1) ))"
      ],
      "metadata": {
        "trusted": true,
        "id": "FYC5LVqvwv3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "HNZPd3EVwv3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WI53ybVAwv3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Build_Model_CNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "    \"\"\"\n",
        "        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n",
        "        word_index in word index ,\n",
        "        embeddings_index is embeddings index, look at data_helper.py\n",
        "        nClasses is number of classes,\n",
        "        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n",
        "        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    embedding_layer = Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True)\n",
        "    # applying a more complex convolutional approach\n",
        "    convs = []\n",
        "    filter_sizes = []\n",
        "    layer = 5\n",
        "    print(\"Filter  \",layer)\n",
        "    for fl in range(0,layer):\n",
        "        filter_sizes.append((fl+2))\n",
        "    node = 128\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    for fsz in filter_sizes:\n",
        "        l_conv = Conv1D(node, kernel_size=fsz, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(5)(l_conv)\n",
        "        #l_pool = Dropout(0.25)(l_pool)\n",
        "        convs.append(l_pool)\n",
        "    l_merge = Concatenate(axis=1)(convs)\n",
        "    l_cov1 = Conv1D(node, 5, activation='relu')(l_merge)\n",
        "    l_cov1 = Dropout(dropout)(l_cov1)\n",
        "    l_pool1 = MaxPooling1D(5)(l_cov1)\n",
        "    l_cov2 = Conv1D(node, 5, activation='relu')(l_pool1)\n",
        "    l_cov2 = Dropout(dropout)(l_cov2)\n",
        "    l_pool2 = MaxPooling1D(30)(l_cov2)\n",
        "    l_flat = Flatten()(l_pool2)\n",
        "    l_dense = Dense(1024, activation='relu')(l_flat)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    l_dense = Dense(512, activation='relu')(l_dense)\n",
        "    l_dense = Dropout(dropout)(l_dense)\n",
        "    preds = Dense(nclasses, activation='softmax')(l_dense)\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "rBhu-tKgwv3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
        "model_CNN.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Qt8AO4evwv3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_CNN.fit(X_train_Glove, y_train,\n",
        "                              validation_data=(X_test_Glove, y_test),\n",
        "                              epochs=15,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yDRZMpq4wv3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model_CNN.predict(X_test_Glove)\n",
        "predicted = np.argmax(predicted, axis=1)\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "metadata": {
        "trusted": true,
        "id": "EPuvL4ebwv3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RCNN"
      ],
      "metadata": {
        "id": "9grRnhsrwv3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_Glove,X_test_Glove, word_index,embeddings_index = loadData_Tokenizer(X_train,X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "21WL5PLewv3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Build_Model_RCNN_Text(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50):\n",
        "    kernel_size = 2\n",
        "    filters = 256\n",
        "    pool_size = 2\n",
        "    gru_node = 256\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) !=len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\",str(len(embedding_matrix[i])),\n",
        "                                 \"into shape\",str(len(embedding_vector)),\" Please make sure your\"\n",
        "                                 \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=pool_size))\n",
        "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
        "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
        "    model.add(LSTM(gru_node, return_sequences=True, recurrent_dropout=0.2))\n",
        "    model.add(LSTM(gru_node, recurrent_dropout=0.2))\n",
        "    model.add(Dense(1024,activation='relu'))\n",
        "    model.add(Dense(nclasses))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "wkTBkz-Vwv3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RCNN = Build_Model_CNN_Text(word_index,embeddings_index, 20)\n",
        "model_RCNN.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IGDUFE0-wv3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_RCNN.fit(X_train_Glove, y_train,\n",
        "                              validation_data=(X_test_Glove, y_test),\n",
        "                              epochs=15,\n",
        "                              batch_size=128,\n",
        "                              verbose=2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w77EzP51wv3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model_RCNN.predict(X_test_Glove)\n",
        "predicted = np.argmax(predicted, axis=1)\n",
        "print(metrics.classification_report(y_test, predicted))"
      ],
      "metadata": {
        "trusted": true,
        "id": "o1uWGFpdwv3u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}